{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "DART_API_KEY = os.getenv(\"DART_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_json = \"https://opendart.fss.or.kr/api/list.json\"\n",
    "params = {\n",
    "    \"crtfc_key\": DART_API_KEY,\n",
    "    \"corp_code\": \"00149655\",\n",
    "    \"bgn_de\": \"20230601\",\n",
    "    \"end_de\": \"20240630\",\n",
    "    # \"pblntf_ty\": \"A\",\n",
    "    # \"pblntf_detail_ty\": \"A001\",\n",
    "}\n",
    "\n",
    "response = requests.get(url_json, params=params)\n",
    "print(response)\n",
    "data = response.json()\n",
    "print(data)\n",
    "data_list = data.get(\"list\")\n",
    "df_list = pd.DataFrame(data_list)\n",
    "print(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_teddynote\n",
    "# %pip install langchain_community\n",
    "# %pip install langchain\n",
    "# %pip install openai\n",
    "# %pip install tiktoken\n",
    "# %pip install langchain_anthropic\n",
    "# %pip install transformers\n",
    "# %pip install PyTorch\n",
    "# %pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"Spoon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from typing import List, Dict, Any\n",
    "from lxml import etree\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import tempfile\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# 상수 정의\n",
    "API_URL = \"https://opendart.fss.or.kr/api/document.xml\"\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 0\n",
    "LLM_MODEL = \"gpt-4-0125-preview\"\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "\n",
    "def fetch_document(api_key: str, rcp_no: str) -> bytes:\n",
    "    \"\"\"DART API를 통해 문서를 가져옵니다.\"\"\"\n",
    "    params = {\"crtfc_key\": api_key, \"rcept_no\": rcp_no}\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def extract_section(\n",
    "    root: etree.Element, start_aassocnote: str, end_aassocnote: str\n",
    ") -> str:\n",
    "    \"\"\"XML에서 특정 섹션을 추출합니다.\"\"\"\n",
    "    start_element = root.xpath(\n",
    "        f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{start_aassocnote}']\"\n",
    "    )[0]\n",
    "    end_element = root.xpath(f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{end_aassocnote}']\")[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "    extracted_elements = []\n",
    "    current_element = start_element\n",
    "    while current_element is not None:\n",
    "        extracted_elements.append(\n",
    "            etree.tostring(current_element, encoding=\"unicode\", with_tail=True)\n",
    "        )\n",
    "        if current_element == end_element:\n",
    "            break\n",
    "        current_element = current_element.getnext()\n",
    "\n",
    "    return \"\".join(extracted_elements)\n",
    "\n",
    "\n",
    "def extract_audit_report(zip_content: bytes, rcp_no: str) -> str:\n",
    "    \"\"\"ZIP 파일에서 감사보고서를 추출합니다.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_content)) as zf:\n",
    "            audit_fnames = [\n",
    "                info.filename\n",
    "                for info in zf.infolist()\n",
    "                if rcp_no in info.filename and info.filename.endswith(\".xml\")\n",
    "            ]\n",
    "            if not audit_fnames:\n",
    "                raise ValueError(\"감사보고서 파일을 찾을 수 없습니다.\")\n",
    "\n",
    "            xml_data = zf.read(audit_fnames[0])\n",
    "            parser = etree.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "            root = etree.fromstring(xml_data, parser)\n",
    "\n",
    "            return extract_section(root, \"D-0-11-2-0\", \"D-0-11-3-0\")\n",
    "\n",
    "    except (zipfile.BadZipFile, etree.XMLSyntaxError, IndexError) as e:\n",
    "        logging.error(f\"감사보고서 추출 실패: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def parse_html_from_xml(xml_data: str) -> etree.Element:\n",
    "    \"\"\"XML 데이터를 HTML로 파싱합니다.\"\"\"\n",
    "    parser = etree.HTMLParser()\n",
    "    return etree.fromstring(f\"<html><body>{xml_data}</body></html>\", parser)\n",
    "\n",
    "\n",
    "def load_html_with_langchain(html_string: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"HTML 문자열을 LangChain 문서로 로드합니다.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", encoding=\"utf-8\", suffix=\".html\", delete=False\n",
    "    ) as temp_file:\n",
    "        temp_file.write(html_string)\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        loader = BSHTMLLoader(temp_file_path, open_encoding=\"utf-8\")\n",
    "        return loader.load()\n",
    "    finally:\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "\n",
    "def tsv_string_to_dataframe(tsv_string: str) -> pd.DataFrame:\n",
    "    \"\"\"TSV 문자열을 pandas DataFrame으로 변환합니다.\"\"\"\n",
    "    return pd.read_csv(io.StringIO(tsv_string), sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"DataFrame의 데이터 타입을 적절히 변환합니다.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if \"금액\" in col or \"건수\" in col:\n",
    "            df[col] = pd.to_numeric(df[col].replace(\"-\", \"0\"), errors=\"coerce\")\n",
    "        elif \"일\" in col:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_report(api_key: str, rcp_no: str) -> pd.DataFrame:\n",
    "    \"\"\"보고서를 요약하여 DataFrame으로 반환합니다.\"\"\"\n",
    "    try:\n",
    "        zip_content = fetch_document(api_key, rcp_no)\n",
    "        logging.info(f\"API 응답 크기: {len(zip_content)} 바이트\")\n",
    "\n",
    "        extracted_content = extract_audit_report(zip_content, rcp_no)\n",
    "        logging.info(\"XML 섹션 추출 완료\")\n",
    "\n",
    "        root = parse_html_from_xml(extracted_content)\n",
    "        logging.info(\"HTML 파싱 완료\")\n",
    "\n",
    "        html_string = etree.tostring(\n",
    "            root, pretty_print=True, method=\"html\", encoding=\"unicode\"\n",
    "        )\n",
    "        docs = load_html_with_langchain(html_string)\n",
    "        logging.info(f\"추출된 문서 수: {len(docs)}\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        template = \"\"\"이 파일에서 '해외채무보증' 또는 '채무보증내역' 제목 아래에 있는 표를 읽어 TSV(Tab-Separated Values) 형식으로 변환해주세요. 다음 지침을 따라주세요:\n",
    "\n",
    "        1. 표의 두 겹 칼럼 구조를 단일 행 헤더로 변환하세요. 상위 칼럼과 하위 칼럼을 언더스코어(_)로 결합하여 새로운 칼럼 이름을 만드세요.\n",
    "           예: '채무보증금액'의 하위 칼럼 '제59기말'은 '채무보증금액_제59기말'로 변환\n",
    "\n",
    "        2. 결과 TSV의 헤더는 다음과 같은 형식이어야 합니다 (실제 칼럼 이름은 원본 표에 따라 다를 수 있음):\n",
    "           성명    관계    채권자    보증건수    보증기간_시작일    보증기간_종료일    채무보증금액_제59기말    채무보증금액_증가    채무보증금액_감소    채무보증금액_제60기말    채무금액\n",
    "\n",
    "        3. 데이터 행은 각 칼럼에 해당하는 값을 포함해야 합니다. 값이 없는 경우 빈 칸으로 두지 말고 '-'로 표시하세요.\n",
    "\n",
    "        4. 숫자 데이터는 쉼표나 기타 구분자 없이 순수한 숫자로 표현하세요.\n",
    "\n",
    "        5. 날짜는 'YYYY-MM-DD' 형식으로 통일하세요.\n",
    "\n",
    "        6. TSV 데이터만 반환하세요. 추가 설명이나 주석은 포함하지 마세요.\n",
    "\n",
    "        7. 결과에 따옴표(''')나 기타 구분자를 포함하지 말고, 순수한 TSV 데이터만 반환하세요.\n",
    "\n",
    "        #Context:\n",
    "        {context}\n",
    "\n",
    "        #Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        llm = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "        context = retriever.get_relevant_documents(\"\")\n",
    "        tsv_result = chain.run(context=context)\n",
    "\n",
    "        df = tsv_string_to_dataframe(tsv_result)\n",
    "        return process_dataframe(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"보고서 요약 중 오류 발생: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = \"b0f7f31f54a0f96561f361c405caa204e64c81a1\"  # DART API 키\n",
    "    rcp_no = \"20240312000736\"  # 문서 번호\n",
    "\n",
    "    try:\n",
    "        result_df = summarize_report(api_key, rcp_no)\n",
    "        print(result_df.head())\n",
    "        print(result_df.dtypes)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"프로그램 실행 중 오류 발생: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fetch_document(api_key, rcp_no):\n",
    "    url = \"https://opendart.fss.or.kr/api/document.xml\"\n",
    "    params = {\"crtfc_key\": api_key, \"rcept_no\": rcp_no}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API 요청 실패: 상태 코드 {response.status_code}\")\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def extract_section(root, start_aassocnote, end_aassocnote):\n",
    "    start_element = root.xpath(\n",
    "        f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{start_aassocnote}']\"\n",
    "    )[0]\n",
    "    end_element = root.xpath(f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{end_aassocnote}']\")[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "    extracted_elements = []\n",
    "    current_element = start_element\n",
    "    while current_element is not None:\n",
    "        extracted_elements.append(\n",
    "            etree.tostring(current_element, encoding=\"unicode\", with_tail=True)\n",
    "        )\n",
    "        if current_element == end_element:\n",
    "            break\n",
    "        current_element = current_element.getnext()\n",
    "\n",
    "    return \"\".join(extracted_elements)\n",
    "\n",
    "\n",
    "def extract_audit_report(zip_content, rcp_no):\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_content)) as zf:\n",
    "            print(\"ZIP 파일 내용:\")\n",
    "            for file_info in zf.infolist():\n",
    "                print(file_info.filename)\n",
    "\n",
    "            audit_fnames = [\n",
    "                info.filename\n",
    "                for info in zf.infolist()\n",
    "                if rcp_no in info.filename and info.filename.endswith(\".xml\")\n",
    "            ]\n",
    "            if not audit_fnames:\n",
    "                raise ValueError(\"감사보고서 파일을 찾을 수 없습니다.\")\n",
    "            xml_data = zf.read(audit_fnames[0])\n",
    "\n",
    "            # XML 파싱\n",
    "            parser = etree.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "            root = etree.fromstring(xml_data, parser)\n",
    "\n",
    "            # 세 부분 추출\n",
    "            part1 = extract_section(root, \"D-0-11-2-0\", \"D-0-11-3-0\")\n",
    "\n",
    "            # 세 부분 합치기\n",
    "            extracted_xml = part1\n",
    "\n",
    "            return extracted_xml\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        raise ValueError(\"ZIP 파일이 손상되었거나 유효하지 않습니다.\")\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        raise ValueError(f\"XML 파싱 실패: {str(e)}\")\n",
    "    except IndexError:\n",
    "        raise ValueError(\"필요한 TITLE 요소를 찾을 수 없습니다.\")\n",
    "\n",
    "\n",
    "def parse_html_from_xml(xml_data):\n",
    "    parser = etree.HTMLParser()\n",
    "    root = etree.fromstring(f\"<html><body>{xml_data}</body></html>\", parser)\n",
    "    print(root)\n",
    "    return root\n",
    "\n",
    "\n",
    "def load_html_with_langchain(html_string):\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", encoding=\"utf-8\", suffix=\".html\", delete=False\n",
    "    ) as temp_file:\n",
    "        temp_file.write(html_string)\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        loader = BSHTMLLoader(temp_file_path, open_encoding=\"utf-8\")\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    finally:\n",
    "        import os\n",
    "\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "\n",
    "def extract_specific_table(html_string, table_title):\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "\n",
    "    # \"채무보증내역\" 제목을 찾습니다\n",
    "    title_element = soup.find(\n",
    "        \"p\", string=lambda text: table_title in text if text else False\n",
    "    )\n",
    "\n",
    "    if title_element:\n",
    "        # 제목 다음에 오는 첫 번째 테이블을 찾습니다\n",
    "        table = title_element.find_next(\"table\")\n",
    "\n",
    "        if table:\n",
    "            # 테이블을 DataFrame으로 변환합니다\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            return df\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def summarize_report(api_key, rcp_no):\n",
    "    # 문서 가져오기\n",
    "    zip_content = fetch_document(api_key, rcp_no)\n",
    "    print(\"API 응답 크기:\", len(zip_content), \"바이트\")\n",
    "\n",
    "    # XML 데이터 추출 및 특정 섹션 파싱\n",
    "    extracted_content = extract_audit_report(zip_content, rcp_no)\n",
    "    print(\"XML 섹션 추출 완료\")\n",
    "\n",
    "    # HTML 파싱\n",
    "    root = parse_html_from_xml(extracted_content)\n",
    "    print(\"HTML 파싱 완료\")\n",
    "\n",
    "    # HTML을 문자열로 변환\n",
    "    html_string = etree.tostring(\n",
    "        root, pretty_print=True, method=\"html\", encoding=\"unicode\"\n",
    "    )\n",
    "\n",
    "    # 특정 테이블 추출\n",
    "    table_df = extract_specific_table(html_string, \"채무보증내역\")\n",
    "\n",
    "    if table_df is not None:\n",
    "        print(\"'채무보증내역' 테이블을 추출했습니다.\")\n",
    "        print(table_df)\n",
    "    else:\n",
    "        print(\"'채무보증내역' 테이블을 찾을 수 없습니다.\")\n",
    "\n",
    "    # LangChain을 사용하여 HTML 로드\n",
    "    docs = load_html_with_langchain(html_string)\n",
    "    print(f\"추출된 문서 수: {len(docs)}\")\n",
    "\n",
    "    return extracted_content, table_df\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "api_key = \"b0f7f31f54a0f96561f361c405caa204e64c81a1\"  # dart api\n",
    "rcp_no = \"20240312000736\"\n",
    "\n",
    "extracted_content, table_df = summarize_report(api_key, rcp_no)\n",
    "\n",
    "# 추출된 XML 데이터를 파일로 저장\n",
    "with open(\"extracted_sections.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(extracted_content)\n",
    "print(\"추출된 섹션들이 'extracted_sections.xml' 파일로 저장되었습니다.\")\n",
    "\n",
    "# 추출된 테이블을 CSV 파일로 저장\n",
    "if table_df is not None:\n",
    "    table_df.to_csv(\"채무보증내역.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"'채무보증내역' 테이블이 '채무보증내역.csv' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV 파일 읽기\n",
    "df = pd.read_csv(\"채무보증내역.tsv\", sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"C:\\\\langchain-kr\\\\01-Basic\\\\Spoon\\\\채무보증내역.csv\", encoding=\"UTF-8\", sep=\",\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -Anthropic 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import tempfile\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def fetch_document(api_key, rcp_no):\n",
    "    url = \"https://opendart.fss.or.kr/api/document.xml\"\n",
    "    params = {\"crtfc_key\": api_key, \"rcept_no\": rcp_no}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API 요청 실패: 상태 코드 {response.status_code}\")\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def extract_section(root, start_aassocnote, end_aassocnote):\n",
    "    start_element = root.xpath(\n",
    "        f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{start_aassocnote}']\"\n",
    "    )[0]\n",
    "    end_element = root.xpath(f\"//TITLE[@ATOC='Y' and @AASSOCNOTE='{end_aassocnote}']\")[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "    extracted_elements = []\n",
    "    current_element = start_element\n",
    "    while current_element is not None:\n",
    "        extracted_elements.append(\n",
    "            etree.tostring(current_element, encoding=\"unicode\", with_tail=True)\n",
    "        )\n",
    "        if current_element == end_element:\n",
    "            break\n",
    "        current_element = current_element.getnext()\n",
    "\n",
    "    return \"\".join(extracted_elements)\n",
    "\n",
    "\n",
    "def extract_audit_report(zip_content, rcp_no):\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_content)) as zf:\n",
    "            print(\"ZIP 파일 내용:\")\n",
    "            for file_info in zf.infolist():\n",
    "                print(file_info.filename)\n",
    "\n",
    "            audit_fnames = [\n",
    "                info.filename\n",
    "                for info in zf.infolist()\n",
    "                if rcp_no in info.filename and info.filename.endswith(\".xml\")\n",
    "            ]\n",
    "            if not audit_fnames:\n",
    "                raise ValueError(\"감사보고서 파일을 찾을 수 없습니다.\")\n",
    "            xml_data = zf.read(audit_fnames[0])\n",
    "\n",
    "            # XML 파싱\n",
    "            parser = etree.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "            root = etree.fromstring(xml_data, parser)\n",
    "\n",
    "            # 세 부분 추출\n",
    "            part1 = extract_section(root, \"D-0-2-0-0\", \"D-0-3-0-0\")\n",
    "            part2 = extract_section(root, \"D-0-3-1-0\", \"D-0-3-2-0\")\n",
    "            part3 = extract_section(root, \"D-0-3-2-0\", \"D-0-3-3-0\")\n",
    "\n",
    "            # 세 부분 합치기\n",
    "            extracted_xml = part1 + part2 + part3\n",
    "\n",
    "            return extracted_xml\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        raise ValueError(\"ZIP 파일이 손상되었거나 유효하지 않습니다.\")\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        raise ValueError(f\"XML 파싱 실패: {str(e)}\")\n",
    "    except IndexError:\n",
    "        raise ValueError(\"필요한 TITLE 요소를 찾을 수 없습니다.\")\n",
    "\n",
    "\n",
    "def parse_html_from_xml(xml_data):\n",
    "    parser = etree.HTMLParser()\n",
    "    root = etree.fromstring(f\"<html><body>{xml_data}</body></html>\", parser)\n",
    "    return root\n",
    "\n",
    "\n",
    "def load_html_with_langchain(html_string):\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", encoding=\"utf-8\", suffix=\".html\", delete=False\n",
    "    ) as temp_file:\n",
    "        temp_file.write(html_string)\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        loader = BSHTMLLoader(temp_file_path, open_encoding=\"utf-8\")\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    finally:\n",
    "        import os\n",
    "\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "\n",
    "# Map 프롬프트 설정\n",
    "map_template = \"\"\"다음은 문서의 일부입니다:\n",
    "{docs}\n",
    "이 부분에서 주요 주제와 재무 정보를 포함한 핵심 내용을 100단어 이내로 요약해주세요.\n",
    "요약:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# LLM 모델 설정 (Claude 3.5 Sonnet 사용)\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce 프롬프트 설정\n",
    "reduce_template = \"\"\"\n",
    "당신은 은행에서 대출을 심사하는 역할입니다.\n",
    "당신은 대출 심사에 대한 판단 전에 신용평가보고서를 작성하고 있습니다.\n",
    "\n",
    "다음은 요약들의 집합입니다: {docs}\n",
    "이것들을 가져다가 최종적으로 통합하여 1.기업체개요 2.산업분석 3.영업현황 및 수익구조 4.재무구조 및 현금흐름 5.신용등급 부여의견으로 구분해서 요약해주세요.\n",
    "각 섹션에 관련 재무 수치를 포함시켜 주세요.\n",
    "\n",
    "예시 :\n",
    "1. 기업체 개요 : 동사 부동산 임대업 등의 사업목적으로 2001.10.16. 설립된 2023년말 기준 총자산 42,615백만원, 자본총계 22,335백만원, 매출액 3,502백만원,\n",
    "당기순이익 37백만원 규모의 외감 소기업임.\n",
    "2. 산업분석 : 최근 전방 산업 경기침체로 공실률 확대 기조 지속되어 매매가력 하락 및 임대소득 하락이 동시에 일어나 부동산 임대업 업황에 부정적인 영향을 미칠 가능성이 높음\n",
    "3. 영업현황 및 수익구조 : 동사 2023년도 기준 매출액 전년도 대비 증가하였는 바, 안정적인 임대수입 영위 중에 있어 향후에도 구준한 매출액 시현에 따른 영업이익 지속 가능시됨.\n",
    "4. 재무구조 및 현금흐름 : 동사 2023년말 기준 차입금 다소 증가하는 등 재무안정성 지표 상 미흡한 수준을 나타내고 있으나, 최근 3년간 무난한 현금흐름 나타내고 있으며, 지속적인 순이익 시현의 내부 유보로 자기자본 규모 확대되고 있음\n",
    "5. 신용등급 부여의견 : 동사 최근 3년간 순이익 지속에 다른 영업활동 상 현금창출 지속되고, 순이익 시현의 내부유보로 자기자본 규모 확대되어 재무구조 개선되고 있으며, 향후에도 안정적인 영업실적 유지에 따른 수익성 유지로 채무상환 능력 인정됨.\n",
    "\n",
    "요약된 내용:\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# Reduce 체인 설정\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# MapReduce 체인 설정\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    document_variable_name=\"docs\",\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "\n",
    "\n",
    "def summarize_report(api_key, rcp_no):\n",
    "    # 문서 가져오기\n",
    "    zip_content = fetch_document(api_key, rcp_no)\n",
    "    print(\"API 응답 크기:\", len(zip_content), \"바이트\")\n",
    "\n",
    "    # XML 데이터 추출 및 특정 섹션 파싱\n",
    "    extracted_content = extract_audit_report(zip_content, rcp_no)\n",
    "    print(\"XML 섹션 추출 완료\")\n",
    "\n",
    "    # HTML 파싱\n",
    "    root = parse_html_from_xml(extracted_content)\n",
    "    print(\"HTML 파싱 완료\")\n",
    "\n",
    "    # HTML을 문자열로 변환\n",
    "    html_string = etree.tostring(\n",
    "        root, pretty_print=True, method=\"html\", encoding=\"unicode\"\n",
    "    )\n",
    "\n",
    "    # LangChain을 사용하여 HTML 로드\n",
    "    docs = load_html_with_langchain(html_string)\n",
    "    print(f\"추출된 문서 수: {len(docs)}\")\n",
    "\n",
    "    # 문서 분할\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    # MapReduce 체인 실행\n",
    "    summary = map_reduce_chain.run(split_docs)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "api_key = \"b0f7f31f54a0f96561f361c405caa204e64c81a1\"\n",
    "rcp_no = \"20240516001638\"\n",
    "\n",
    "summary = summarize_report(api_key, rcp_no)\n",
    "print(summary)\n",
    "\n",
    "# 추출된 XML 데이터를 파일로 저장 (옵션)\n",
    "# with open(\"extracted_sections.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(extracted_content)\n",
    "# print(\"추출된 섹션들이 'extracted_sections.xml' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-ZEAO8RS5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
